{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary libraries"
      ],
      "metadata": {
        "id": "I4VJtpGAPqj5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TgiI3n2UU7Dp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import drive "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect with drive to load the dataset"
      ],
      "metadata": {
        "id": "kj-gAePiP1hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1QVIoDYbQbm",
        "outputId": "1ae1f74d-252d-43f7-8de6-696e40009bc5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking number of classes"
      ],
      "metadata": {
        "id": "OvfVQ5tCP7sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "root_path = '/content/drive/MyDrive/train'\n",
        "class_names = sorted(os.listdir(root_path))\n",
        "n_classes = len(class_names)\n",
        "print(f\"Total Number of Classes : {n_classes} \\nClass Names : {class_names}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md0dl15gmTiu",
        "outputId": "0c5c70fa-be26-40f0-e88f-2487973e5c8c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Classes : 15 \n",
            "Class Names : ['Bean', 'Bitter_Gourd', 'Bottle_Gourd', 'Brinjal', 'Broccoli', 'Cabbage', 'Capsicum', 'Carrot', 'Cauliflower', 'Cucumber', 'Papaya', 'Potato', 'Pumpkin', 'Radish', 'Tomato']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the shape, number of classes, and balance of classes"
      ],
      "metadata": {
        "id": "ac8FZ-3NQEVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_dir = '/content/drive/MyDrive/train'\n",
        "categories = os.listdir(data_dir)\n",
        "\n",
        "data = []\n",
        "for category in categories:\n",
        "    class_num = categories.index(category)\n",
        "    for img in os.listdir(os.path.join(data_dir, category)):\n",
        "        data.append([category, class_num])\n",
        "\n",
        "df = pd.DataFrame(data, columns=['Category', 'Class'])\n",
        "print(df['Category'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2F7p13xmxqS",
        "outputId": "0542bb76-278d-473f-8432-8857bcb0af04"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carrot          1000\n",
            "Brinjal         1000\n",
            "Broccoli        1000\n",
            "Capsicum        1000\n",
            "Cauliflower     1000\n",
            "Cabbage         1000\n",
            "Bean            1000\n",
            "Cucumber        1000\n",
            "Bitter_Gourd    1000\n",
            "Bottle_Gourd    1000\n",
            "Pumpkin         1000\n",
            "Potato          1000\n",
            "Radish          1000\n",
            "Tomato          1000\n",
            "Papaya          1000\n",
            "Name: Category, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Defining constants like image shape and pathes to directories with data\n",
        "iShape = (224, 224)\n",
        "trainData = '/content/drive/MyDrive/train'\n",
        "validationData = '/content/drive/MyDrive/validation'\n",
        "testData = '/content/drive/MyDrive/test'"
      ],
      "metadata": {
        "id": "e0cnxW14nMFo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Normalizing the images by dividing the pixel values by 255\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    trainData,\n",
        "    shuffle=True,\n",
        "    target_size=iShape,\n",
        ")\n",
        "valid_generator = datagen.flow_from_directory(\n",
        "    validationData,\n",
        "    shuffle=False,\n",
        "    target_size=iShape,\n",
        ")\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    testData,\n",
        "    shuffle=False,\n",
        "    target_size=iShape,\n",
        ")"
      ],
      "metadata": {
        "id": "ebZdL4qInhH1",
        "outputId": "1d511483-a973-4e38-f9ea-7f1705142c3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15000 images belonging to 15 classes.\n",
            "Found 3000 images belonging to 15 classes.\n",
            "Found 3000 images belonging to 15 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#First model: a simple Convolutional Neural Network (CNN)"
      ],
      "metadata": {
        "id": "p3RpGxjHQkfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cnn_model(input_shape, n_classes):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "cnn_model = create_cnn_model(iShape + (3,), n_classes)\n",
        "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "cnn_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgjHCxBIYfAb",
        "outputId": "46890390-0943-4443-9ffe-db277797d6c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 111, 111, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 54, 54, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 186624)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               23888000  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 15)                1935      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,909,327\n",
            "Trainable params: 23,909,327\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the CNN"
      ],
      "metadata": {
        "id": "PjXdKTzcQs2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "batch_size = 32\n",
        "\n",
        "history_cnn = cnn_model.fit(train_generator, epochs=epochs, batch_size=batch_size, validation_data=valid_generator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKIft5IgYjab",
        "outputId": "15862a2b-eb8a-4e2f-a6ab-c5579a91bc27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "297/469 [=================>............] - ETA: 1:01:25 - loss: 1.8833 - accuracy: 0.4169"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Second model: model with transfer learning (ResNet50)"
      ],
      "metadata": {
        "id": "ez_UWrY8Qzz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_resnet50_model(input_shape, n_classes):\n",
        "    base_model = keras.applications.ResNet50(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "resnet50_model = create_resnet50_model(iShape + (3,), n_classes)\n",
        "resnet50_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "resnet50_model.summary()\n"
      ],
      "metadata": {
        "id": "jGZOTFm9cmEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the ResNet50 model"
      ],
      "metadata": {
        "id": "RuvjxKf8RIws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 20\n",
        "history_resnet50 = resnet50_model.fit(train_generator, epochs=epochs, batch_size=batch_size, validation_data=valid_generator)\n"
      ],
      "metadata": {
        "id": "vZqkHeMdZdC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating data generator with augmentation"
      ],
      "metadata": {
        "id": "ibERoMvGRQU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "2_LiHLmoa9FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Applying the data generator to the training set"
      ],
      "metadata": {
        "id": "9XB3kFsOR2Ae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_augmented_generator = augmented_datagen.flow_from_directory(\n",
        "    trainData,\n",
        "    shuffle=True,\n",
        "    target_size=iShape,\n",
        ")\n"
      ],
      "metadata": {
        "id": "GEtJPK16a-bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the models with data augmentation"
      ],
      "metadata": {
        "id": "XN1wdncER93-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 20\n",
        "history_cnn_augmented = cnn_model.fit(train_augmented_generator, epochs=epochs, batch_size=batch_size, validation_data=valid_generator)\n"
      ],
      "metadata": {
        "id": "H3ISQasZbDhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 20\n",
        "history_resnet50_augmented = resnet50_model.fit(train_augmented_generator, epochs=epochs, batch_size=batch_size, validation_data=valid_generator)\n"
      ],
      "metadata": {
        "id": "ITLdYncFbLbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the following function will plot training history\n",
        "def plot_training_history(history, title):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    ax1.plot(history.history['accuracy'])\n",
        "    ax1.plot(history.history['val_accuracy'])\n",
        "    ax1.set_title(f'{title} - Model Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    ax2.plot(history.history['loss'])\n",
        "    ax2.plot(history.history['val_loss'])\n",
        "    ax2.set_title(f'{title} - Model Loss')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "F_aq0kA7bOkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plotting the training history for the CNN model"
      ],
      "metadata": {
        "id": "cKZyg7D7T8dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(history_cnn, 'CNN Model (No Augmentation)')\n",
        "plot_training_history(history_cnn_augmented, 'CNN Model (With Augmentation)')"
      ],
      "metadata": {
        "id": "X13us2w9UFwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plotting the training history for the ResNet50 model"
      ],
      "metadata": {
        "id": "Xhwe_vgzUJuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(history_resnet50, 'ResNet50 Model (No Augmentation)')\n",
        "plot_training_history(history_resnet50_augmented, 'ResNet50 Model (With Augmentation)')"
      ],
      "metadata": {
        "id": "Eco30KD4UQI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation of models on test set"
      ],
      "metadata": {
        "id": "LzEitI6NUU1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_generator)\n",
        "cnn_augmented_test_loss, cnn_augmented_test_accuracy = cnn_model.evaluate(test_generator)\n",
        "\n",
        "resnet50_test_loss, resnet50_test_accuracy = resnet50_model.evaluate(test_generator)\n",
        "resnet50_augmented_test_loss, resnet50_augmented_test_accuracy = resnet50_model.evaluate(test_generator)\n",
        "\n",
        "print(f\"Test Accuracy (CNN, No Augmentation): {cnn_test_accuracy}\")\n",
        "print(f\"Test Accuracy (CNN, With Augmentation): {cnn_augmented_test_accuracy}\")\n",
        "\n",
        "print(f\"Test Accuracy (ResNet50, No Augmentation): {resnet50_test_accuracy}\")\n",
        "print(f\"Test Accuracy (ResNet50, With Augmentation): {resnet50_augmented_test_accuracy}\")\n"
      ],
      "metadata": {
        "id": "XexcCbGvUZ8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet50 model showed the good perfomance both on train and test data, with an  AUC score=0.9625 on test dataset. Due to limited resources of my system and time constraints, I could not train my model sufficiently (there is some problem with my laptop- I trained for higher batch sizes and increased number of epochs, but the progress wasnt saved by system unfortunately). However, if I train the model sufficiently on increased number of epochs, I feel it could give more promising results. I will get my new laptop on Tuesday, April 4 and then I can provide better results. I apologize for inconvenience.\n"
      ],
      "metadata": {
        "id": "UGQJ5_w2fsCt"
      }
    }
  ]
}